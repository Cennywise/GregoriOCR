Team: Janice Adams, Keiran Hozven-Farley, Tori Tomlinson

***
# **Project: Corpus of Chords**
***
 * Data:  
    * Our project uses a dataset of [Bach Chorales](https://github.com/czhuang/JSB-Chorales-dataset) hosted on Github in MIDI note numbers.
    * This dataset translates all of the chords in the songs into 4 Dimensional vectors.
    * Under the folder Data\Corpi, we remove any repeated chords in all of the songs.     
      
 * Word2Vec:  
    * We use an implementation of the Word2Vec algorithms modeled on this [tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec).
    https://www.tensorflow.org/tutorials/text/word2vec
    * word2vec is an algorithm that generates n dimensional representations of words such that related chords are close to eacher in
    euclidian space. It does this by iterating through the dataset to find positive and negative skip grams, and posing a catagorization problem between them by taking the dot product of the pairs and seeing if it is sufficiently small. The Weights generated by the embedings layer of the model (the one that translates between psuedo one-hot encodings of the word and vectors in n-space) then become our chord vectors.

 * Generator 
    * once a set of embeddings is obtained, we generate new sequences by assosiating each embedding vector with its N closest         neighbors, Choosing a starting vector arbitrarily, than iterativly tracing a path through the embedding space by moving to neighbors. The model cannot account for rhythem, however by holding notes whenever they are repeated in adjacent chords we end up with some rhythem as an emergant property.

 * K-cluster:  
    * We used two K-cluster algorithms - one is a student-implemented algorithm from MAT167 HW4 (Chandler's summer 2022 course), the other is a pre-built implementation from sklearn. Both are run on the embeddings list outputted from Word2Vec. 
    * Then we match up the partitioned weights with information from music21 to view the chord names, root tones, and quality (major/minor/etc) in each partition. 
      
 * Tables:  
   * Our tables list the partitions, and for each partition displays the top n most common chords given a feature (chord name, chord root, or chord quality). 
   * See 'Analysis' 
   * See the text files in the OutputAnalysis folder for the following tables (ran from a word2vec with dimension=128 and negative sampling=15)(k=number of clusters):
      * KmeansStudent_ChordName_d_128_n_15_k_24
      * KmeansStudent_Root_d_128_n_15_k_24
      * KmeansStudent_Quality_d_128_n_15_k_24
      * SKlearn_ChordName_d_128_n_15_k_24
      * SKlearn_Root_d_128_n_15_k_24
      * SKlearn_Quality_d_128_n_15_k_24
      * SKlearn_ChordName_128_n_15_k_12
      * SKlearn_ChordName_d_128_n_15_k_48
      
    
## **How to use this machine learning project**  

   1. Data (Instructions state that user should be able to use their own data).  
   DATA MUST BE IN SAME FORMAT AS JSB CHORALS DATASET, FOUR NOTE CHORDS, -1 FOR EMPTY NOTES
      
   Install Python libraries
    
		tabulate
		numpy
		tensorflow
		tensorboard
		keras
		sklearn (aka scikit-learn)
		mido
      music21

   2. navigate to the Data/Corpi/ folder to find a dataset to run on (alternativly use the buildDataset function in playground.py 
   to generate your own varient on the dataset "Jsb16thSeperated(RAW)) DO NOT RUN MODEL ON THE RAW DATASET, IT HAS NOT BEEN PREPROSESSED



   3. Open the word2vec_notebook , and follow the instructions written therein to train a model on your chosen dataset, and save the outputs to disc.

   4. from here there are three things you can do with the generated embeddings

      a. visualise the data in the tensorboard embeddings projector. Run tensorboard from the apropriate tensorboard data folder, located in the folder assosiated with your dataset. YOU MUST PLACE THE METADATA GENERATED BY buildDataset INTO THIS FOLDER YOURSELF, before running tensorboard, in order to get labels on your embedings 

      b. generate new sequence. run generationModel in playground.py, feeding it the name of the dataset you used and the name of the model you generated. Next run generateSequence, feeding it the length of the sequence and the same information you fed generationModel, a midi file will be output into the root project directory. 

      c. clustering
  
 * Clone the corpus. The following Jupyter notebooks are in the folder Scrips:
   * word2vec_notebook.ipynb
   * KmeansSklearn.ipynb
   * KmeansStudentAlgorithm.ipynb
 

 * If you want to run on our prepared (currently set) dataset, simply open one of the notebooks and run. Note that 
 *    
 * word2vec_notebook.ipynb
 * Open and run

FILE FOLDER DISCLAIMER

word2vec_notebook.ipynb:
   * Open and run the word2vec_notebook.ipynb
K
generator
   


##
