Team: Janice Adams, Keiran Hozven-Farley, Tori Tomlinson

***
# **Project: Corpus of Chords**
***
 * Data:  
    * Our project uses a dataset of [Bach Chorales](https://github.com/czhuang/JSB-Chorales-dataset) hosted on Github in MIDI note numbers.
    * This dataset translates all of the chords in the songs into 4 Dimensional vectors.
    * Under the folder Data\Corpi, we remove any repeated chords in all of the songs.   
    * Then Jsb_Vocab .  
      
 * Word2Vec:  
    * We use an implementation of the Word2Vec algorithms modeled on this tensorflow tutorial.
    https://www.tensorflow.org/tutorials/text/word2vec
    * word2vec is an algorithm that generates n dimentional representations of words such that related chords are close to eacher in
    euclidian space. It does this iterating through the dataset to find positive and negative skip grahms, and posing a catagorization problem between them by taking the dot product of the pairs and seeing if it is sufficiently small. The Weights generated by the embedings layer of the model (the one that translates between psuedo one-hot encodings of the word and vectors in n-space) than become our chord vectors.

 * Generator 
    * once a set of embeddings is obtained, we generate new sequences by assosiating each embedding vector with its N closest         neighbors, Choosing a starting vector arbitrarily, than iterativly tracing a path through the embedding space by moving to neighbors. The model cannot account for rhythem, however by holding notes whenever they are repeated in adjacent chords we end up with some rhythem as an emergant property.

 * K-cluster:  
    * We used two K-cluster algorithms - one is a student-implemented algorithm from MAT167 HW4 (Chandler's summer 2022 course), the other is a pre-built implementation from sklearn. Both are run on the embeddings list outputted from Word2Vec. 
    * Then we match up the partitioned weights with information from music21 to view the chord names, root tones, and quality (major/minor/etc) in each partition. 
      
 * Tables:  
   * Our tables list the partitions, and for each partition displays the top n most common chords given a feature (chord name, chord root, or chord quality). 
   * See 'Analysis' 
   * See the text files in the OutputAnalysis folder for the following tables (ran from a word2vec with dimension=128 and negative sampling=15)(k=number of clusters):
      * KmeansStudent_ChordName_d_128_n_15_k_24
      * KmeansStudent_Root_d_128_n_15_k_24
      * KmeansStudent_Quality_d_128_n_15_k_24
      * SKlearn_ChordName_d_128_n_15_k_24
      * SKlearn_Root_d_128_n_15_k_24
      * SKlearn_Quality_d_128_n_15_k_24
      * SKlearn_ChordName_128_n_15_k_12
      * SKlearn_ChordName_d_128_n_15_k_48
      
    
## **How to use this machine learning project**  

   1. Data (Instructions state that user should be able to use their own data).  
      
   Import Python libraries
      os
		io
		math
		random
		tabulate
		collections
		json
		numpy
		tensorflow
		tensorboard
		keras
		sklearn (aka scikit-learn)
		mido
      music21


   3. Run the Jupyter notebooks
  
 * Clone the corpus. The following Jupyter notebooks are in the folder Scrips:
   * word2vec_notebook.ipynb
   * KmeansSklearn.ipynb
   * KmeansStudentAlgorithm.ipynb
 

 * If you want to run on our prepared (currently set) dataset, simply open one of the notebooks and run. Note that 
 *    
 * word2vec_notebook.ipynb
 * Open and run

FILE FOLDER DISCLAIMER

word2vec_notebook.ipynb:
   * Open and run the word2vec_notebook.ipynb
K
generator
   


##
