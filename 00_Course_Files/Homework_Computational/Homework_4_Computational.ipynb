{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034abf95",
   "metadata": {},
   "source": [
    "# <center> Computational Homework 4</center>\n",
    "\n",
    "First I provide some functions which we will use often to plot various things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48add7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import needed modules and define plotting functions (no input needed)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import random, time\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.datasets import fetch_openml\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "# we will use this later to save weights as JSON files\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "# function to plot costs\n",
    "def plot_costs(costs):\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"cost\")\n",
    "    plt.show()\n",
    "    \n",
    "# function to plot costs\n",
    "def plot_grads(grads):\n",
    "    plt.plot(grads)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"gradient norm\")\n",
    "    plt.show()\n",
    "\n",
    "# defining a function to plot data\n",
    "def plot_data(X,Y,size_ = 2):\n",
    "    m = len(X)\n",
    "    plot_figure = go.Figure(data=[go.Scatter3d(x=X[:,0], y=X[:,1], z=[r[0] for r in Y], mode='markers',marker=dict(size=size_))])\n",
    "    plotly.offline.iplot(plot_figure)\n",
    "\n",
    "# defining a function to plot models fit\n",
    "def plot_fit(X,Y,W,B,G,size_ = 2):\n",
    "    trace = go.Scatter3d(x=X[:,0], y=X[:,1], z=[r[0] for r in Y], mode='markers',marker=dict(size=size_))\n",
    "    xs,ys = X[:,0],X[:,1]\n",
    "    xxx = np.outer(np.linspace(min(xs), max(xs), 30), np.ones(30))\n",
    "    yyy = np.outer(np.linspace(min(ys), max(ys), 30), np.ones(30)).T\n",
    "    zzz = np.zeros([30,30])\n",
    "    D = len(G)-1\n",
    "    for i in range(30):\n",
    "        for j in range(30):\n",
    "            zzz[i,j] = feedforward(W,B,G,np.array([xxx[i,j],yyy[i,j]]))[D][0]\n",
    "    # Configure the layout.\n",
    "    layout = go.Layout(margin={'l': 0, 'r': 0, 'b': 0, 't': 0})\n",
    "    data = [trace,go.Surface(x=xxx, y=yyy, z=zzz, showscale=False, opacity=0.5)]\n",
    "    # Render the plot.\n",
    "    plot_figure = go.Figure(data=data, layout=layout)\n",
    "    plot_figure.update_layout(\n",
    "        scene = dict(\n",
    "            xaxis = dict(nticks=4, range=[min(X[:,0]),max(X[:,0])],),\n",
    "            yaxis = dict(nticks=4, range=[min(X[:,1]),max(X[:,1])],),\n",
    "            zaxis = dict(nticks=4, range=[min(Y),max(Y)],),),\n",
    "        width=700,\n",
    "        margin=dict(r=20, l=10, b=10, t=10))\n",
    "    plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9e5e7",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "You should do problem 1 and 2 of Theoretical Homework 4 before doing this problem.\n",
    "\n",
    "Define activation functions `ReLU`,`Linear`,`Sigmoid` `Squared`, `Softmax` (where `Squared` should square each component). Each should be able to take a numpy array as input and output a numpy array. For each function, we want the option of using the function itself or its derivative, so each function should have an extra input `deriv` with default value `False` but if `deriv` is set to `True`, then the output is the derivative of the activation function evaluated at the input `x` (this should be a square matrix in every case even if it is 1 by 1).\n",
    "\n",
    "Also define a function `loss` with inputs `Nx`, `y`, `cost_type = 'se'` and `deriv = False` which outputs the per-example loss function if `deriv = False` or the derivative of the loss function with respect to `Nx` if `deriv = True`. In the case that `deriv = True` the function should output a vector and otherwise the output should be a number. The input `cost_type` has three possible values, `se`, `ce`, and `bce` for squared error, cross-entropy, and binary cross-entropy as discussed in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aed056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to complete problem 1, fill in the following with your code\n",
    "\n",
    "def ReLU(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "\n",
    "def Linear(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "\n",
    "def Sigmoid(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "\n",
    "def Squared(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "\n",
    "def Softmax(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "\n",
    "# Note: Nx and y are always numpy arrays (for 'bce' they always have only one entry)\n",
    "# when deriv = False the output must be a number and when deriv = True the output must be a vector\n",
    "def loss(Nx,y,cost_type,deriv = False):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df076d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the answers are correct for yourself (no input needed)\n",
    "print(f'ReLU([1,1,-1]): {ReLU(np.array([1,1,-1]))}\\n')\n",
    "print(f'Linear([1,1,-1]): {Linear(np.array([1,1,-1]))}\\n')\n",
    "print(f'Sigmoid([1,1,-1]): {Sigmoid(np.array([1,1,-1]))}\\n')\n",
    "print(f'Squared([1,2,-3]): {Squared(np.array([1,2,-3]))}\\n')\n",
    "print(f'Softmax([1,2,-3]): {Softmax(np.array([1,2,-3]))}\\n')\n",
    "print(f'(deriv) ReLU([1,1,-1]): \\n{ReLU(np.array([1,1,-1]),True)}\\n')\n",
    "print(f'(deriv) Linear([1,1,-1]): \\n{Linear(np.array([1,1,-1]),True)}\\n')\n",
    "print(f'(deriv) Sigmoid([1,1,-1]): \\n{Sigmoid(np.array([1,1,-1]),True)}\\n')\n",
    "print(f'(deriv) Squared([1,2,-3]): \\n{Squared(np.array([1,2,-3]),True)}\\n')\n",
    "print(f'(deriv) Softmax([1,2,-3]): \\n{Softmax(np.array([1,2,-3]),True)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a277f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that your loss functions are working (no input needed)\n",
    "# answers:\n",
    "#0.29999999999999993\n",
    "#[-0.2 -1.   0.4]\n",
    "#0.7985076962177715\n",
    "#[-1.11111111 -2.          0.        ]\n",
    "#0.10536051565782628\n",
    "#[-1.11111111]\n",
    "Nx = np.array([.9,.5,.2])\n",
    "y = np.array([1,1,0])\n",
    "print(f\"loss(Nx,y,se):                       {loss(Nx,y,'se')}\")\n",
    "print(f\"loss(Nx,y,se,deriv = True):          {loss(Nx,y,'se',deriv = True)}\")\n",
    "print(f\"loss(Nx,y,ce):                       {loss(Nx,y,'ce')}\")\n",
    "print(f\"loss(Nx,y,ce,deriv = True):          {loss(Nx,y,'ce',deriv = True)}\")\n",
    "Nx = np.array([.9])\n",
    "y = np.array([1])\n",
    "print(f\"loss(Nx,y,bce):                      {loss(Nx,y,'bce')}\")\n",
    "print(f\"loss(Nx,y,bce,deriv = True):         {loss(Nx,y,'bce',deriv = True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0639ed0",
   "metadata": {},
   "source": [
    "# Problems 2 - 4\n",
    "\n",
    "The purpose of problems 2 through 4 is to implement forward propagation and backpropagation to fit a neural network to a dataset `X,Y` using stochastic gradient descent with momentum and regularization. We will implement backpropagation to fit weights `W,B` for a neural network with architecture $\\mathcal{H} = P(n_0,...,n_D)$ (all possible connections between layers of sizes $n_0,...,n_D$). \n",
    "\n",
    "Coding a neural network is very prone to error so in each function we will include a parameter `verbose` with default value `False` which we will use to switch on and off print statements which you can use to debug your functions along the way. I will not dictate which print statements you use for debugging, but instead suggest you include whichever print statements you desire only after `if verbose:` so you have the option to switch them on or off as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780894e",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "To start, we define a function `feedforward(W,B,G,x)` where \n",
    "\n",
    "- `W` is a list of matrices with the property that `W[i+1].shape[1] == W[i].shape[0]` for all `i`\n",
    "- `B` is a list of vectors with the property that `B[i].shape == W[i].shape[0]` for all `i`\n",
    "- `G` is a list of activations with length `len(W)+1` where `G[i]` is a function which takes a numpy array of dimension `W[i].shape[0]` as input and gives a numpy array of the same dimension as output. We always take `g[0] = Linear` (usually we would not have an activation here at all but it is convenient to use `Linear` here just as a place holder. Think of it as being the same as not having an activation function at layer 0.)\n",
    "- `x` is an input vector of dimension `W[0].shape[1]`\n",
    "\n",
    "the output is a list `feeds` where `feeds[i]` is the pair `[xi,si]` where `x0 = x`, `s0 = W[0]@x+B[0]`, and for `i>0` we have `x(i+1) = G[i+1](si)` and `si = W[i]@xi+B[i]`. The last entry only has an `x` and no signal (this is the feedforward `Nx` of the network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a05452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(W,B,G,x):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(no input needed)\n",
    "#test output (output should be [array([ 1.,  4.,  9., 16., 25.])])\n",
    "G,arch = [Linear,Softmax,Sigmoid,Sigmoid,Softmax,Squared],[10,20,20,20,20,5]\n",
    "W,B,D = [],[],len(G)-1\n",
    "for l in range(D):\n",
    "    W.append(np.ones((arch[l+1], arch[l])))\n",
    "    B.append(np.array(range(arch[l+1])))\n",
    "x1 = np.ones(W[0].shape[1])\n",
    "x2 = np.zeros(W[0].shape[1])\n",
    "X_feeds = {0:feedforward(W,B,G,x1),1:feedforward(W,B,G,x2)}\n",
    "X_feeds[0][D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30305909",
   "metadata": {},
   "source": [
    "# Problem 2 Continued...\n",
    "\n",
    "Given a dataset `X,Y`, to compute the $\\delta$ values for a batch, we first compute all of the feedforwards of each example in the batch. Suppose we have already done this and saved the feedforwards in a dictionary `X_feeds` where `X_feeds.keys()` is a subset (corresponding to the given batch) of `{1,...,m}` and `X_feeds[i] = feedforward(W,B,G,X[i,:])`. \n",
    "\n",
    "In this problem, we write a function `deltas(X_feeds,Y,W,B,G,verbose = False, cost_type = 'se')` which returns `deltas_dict` which is a dictionary having keys `i for i in X_feeds.keys()` and values $\\delta_i^{(0)},...,\\delta_i^{(D-1)}$. For this problem, we assume the batch is already chosen and `X_feeds` has already been computed (you will compute `X_feeds` before calling `deltas` in the following problem).\n",
    "\n",
    "For each `i in X_feeds.keys()` start by computing $\\delta_i^{(D-1)}$ and then compute $\\delta_i^{(D-2)}$, $\\delta_i^{(D-3)}$, and so on until $\\delta_i^{(0)}$ as described in lecture. To compute $\\delta_i^{(D-1)}$ you will use the `loss` function defined above with `deriv = True`.  You will also require the use of the derivatives of activations evaluated at some signal `sl` which you can call with `G[l](sl,deriv = True)`. Make sure that for each `i in batch`, the value `deltas_dict[i]` is a list $\\delta_i^{(0)},...,\\delta_i^{(D-1)}$ in that order. Since you computed the list in reverse order, you may want to use the `.reverse()` method to reverse the order.\n",
    "\n",
    "Debugging: it may be a good idea to print the shapes of the quantities along the way. Often if there is a computational error it will be because you are attempting to multiply things with different shapes. You may want to print these shapes if `verbose` is set to `True`. Then if you run into an error, the last thing printed is usually causing the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf4f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def deltas(X_feeds,Y,W,B,G,verbose = False, cost_type = 'se'):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no input needed\n",
    "# should get array([  0.,  16.,  72., 192., 400.])\n",
    "Y = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]])\n",
    "deltas(X_feeds,Y,W,B,G,verbose = False, cost_type = 'se')[0][D-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f462c",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Make sure to do problem 3 of Theoretical Homework 4 before doing this problem (needed for the regularization part).\n",
    "\n",
    "We will now approximate (using the ideas of SGD) the gradient of the cost function\n",
    "\n",
    "$$J(W,B) = \\frac{1}{m}\\sum_{i}\\mathrm{loss}(Nx_i,y_i)$$\n",
    "\n",
    "Write a function `grads(X,Y,W,B,G,batch, lambda_ = 0, verbose = False,cost_type = 'se')` which returns `dWs,dBs,X_feeds` where `dWs[l]` is the value of $\\frac{\\partial J}{\\partial W^{(l)}}$ and `dBs[l]` is the value of $\\frac{\\partial J}{\\partial b^{(l)}}$ and `X_feeds` is a dictionary with keys `i for i in batch` and values `X_feeds[i]=feedforward(W,B,G,xi)` where `xi` is the row `i` of the matrix `X`. \n",
    "\n",
    "To compute the entries of `dWs` and `dBs` you should call `X_deltas = deltas(X_feeds,Y,W,B,G,verbose,cost_type)` and use the values from the `deltas` function to compute the gradients as explained in lecture. The `dWs` should also have a regularization term depending on `lambda_` (see problem 3 of Homework 4 Theoretical).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f20f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads(X,Y,W,B,G,batch, lambda_ = 0, verbose = False,cost_type = 'se'):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7dfda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should get the following:\n",
    "#array([-7.08328564e-09, -1.92543667e-08, -5.23387950e-08, -1.42271595e-07,\n",
    "#       -3.86734292e-07, -1.05125280e-06, -2.85760138e-06, -7.76776591e-06,\n",
    "#       -2.11149769e-05, -5.73964581e-05, -1.56019749e-04, -4.24105649e-04,\n",
    "#       -1.15283868e-03, -3.13374043e-03, -8.51838966e-03, -2.31553838e-02,\n",
    "#       -6.29428591e-02, -1.71096430e-01, -4.65088317e-01, -1.26424112e+00])\n",
    "X = np.array([x1,x2])\n",
    "batch = [0,1]\n",
    "grads(X,Y,W,B,G,batch, lambda_ = 0, verbose = False,cost_type = 'se')[0][D-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df5b6d",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Write a function `fit(X,Y,arch,G,alpha = 1e-9, momentum = .01, batch_size = 100, lambda_ = 0, max_iters = 100,verbose = False, cost_type = 'se',print_costs = True)` which returns `W,B,costs,grad_norms` where `W` is a list of the weight matrices $W^{(l)}$ and $B$ is a list of bias vectors $B^{(l)}$ for $0\\leq l\\leq D-1$ which we find after running `max_iters` epochs of stochastic gradient descent. \n",
    "\n",
    "* `arch` is a list of layer sizes `arch = [n0,...,nD]`.\n",
    "* `momentum` is the momentum coefficient from the momentumm method\n",
    "* `print_costs` should be set to `True` if you want to print the cost after a certain number of epochs to see your progress. Sometimes it is nice to be able to turn this function off.\n",
    "\n",
    "Start by defining `W,B,VW,VB = [],[],[],[]` and `D,m = len(G)-1,len(X)`. Then for `l in range(D)` append initializations to `W,B,VW,VB` as indicated in lecture. Introduce variables `epochs,costs,grad_norms` to keep track of the number of iterations, the cost, and the norm of the gradient. The main loop of gradient descent will start with `while epochs<=max_iters:` though you can experiment with different stopping conditions if you would like to. At each step you should:\n",
    "\n",
    "* pick a batch of size `batch_size` using the `random.sample` method\n",
    "* compute `dWs,dBs,feeds = grads(X,Y,W,B,G,batch,lambda_,verbose,cost_type)`\n",
    "* compute the norm of the current gradient by adding the norms of each weight matrix and bias vector together and append this value to `grad_norms`\n",
    "* approximate the current cost by computing the average `loss(Nx,Y[i],cost_type)` where `Nx = feeds[i][D][0]` for `i in batch` and append this to `costs`.\n",
    "* `if epochs%(np.floor(max_iters/30))==0 and print_costs:` you should `print(f'epoch: {epochs}')` and `print(f'           cost: {cost}')` including extra spaces so the indentation makes things easier to look at\n",
    "* update all of the values of `W,B,VW,VB` using the momentum method discussed in lecture\n",
    "* increment `epochs` by 1\n",
    "\n",
    "\n",
    "Optional: If you want to add an option to decrease the size of `alpha` over time you may do this. If so it might be smart to include a condition which signals you to start the decrease, like once the cost function gets below a given threshold. This threshold can even be another hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iters should be a multiple of 100\n",
    "def fit(X,Y,arch,G,alpha = 1e-9, momentum = .01, batch_size = 100, \n",
    "        lambda_ = 0, max_iters = 100,verbose = False, cost_type = 'se',print_costs = True):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be35ae",
   "metadata": {},
   "source": [
    "To test your backprop function, lets see if we can fit a linear function with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a dataset (no input needed)\n",
    "X = np.random.rand(10000,2)*50 - np.random.rand(10000,2)*100\n",
    "def p_data(p):\n",
    "    return p[0] - p[1] + np.random.normal(0,10)\n",
    "Y = np.array([[p_data(p)] for p in X])\n",
    "plot_data(X,Y,size_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an architecture arch with a single hidden layer and activations G\n",
    "# both arch and G should be lists of size 3 (input, hidden, output)\n",
    "# we always pick Linear for the input activation and the rest are your choice\n",
    "arch = \n",
    "G = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e26e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run backpropagation after choosing alpha, batch_size, lambda_ and max_iters\n",
    "W,B,costs,grad_norms = fit(X,Y,arch,G,alpha = 2e-1,batch_size = 500, lambda_ = 10, max_iters = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the costs  (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no input needed\n",
    "# plot the gradient norms (don't expect this to go to 0... we do not in general find a local minimum)\n",
    "# plotting the gradient norms sometimes lets us know if our fit is failing because we are getting stuck\n",
    "# in a bad local minimum (turns out this is rare)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3648385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well your result fits the training set (no input needed)\n",
    "plot_fit(X,Y,W,B,G,size_ = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9997d4f",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "The goal of this problem is to fit a quadradic function. All you have to do is pick an architecture and hyperparameters. Use at least 3 different activation functions and at least 2 hidden layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36aa5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a dataset (no input needed)\n",
    "X = np.random.rand(10000,2)*50 - np.random.rand(10000,2)*100\n",
    "def p_data(p):\n",
    "    return p[0]**2 - p[1]**2 + np.random.normal(0,500)\n",
    "Y = np.array([[p_data(p)] for p in X])\n",
    "plot_data(X,Y,size_ = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b780b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose your architecture, activations, and hyperparameters\n",
    "# the default values given in this cell will not work well so you must make changes to get a good fit\n",
    "# suggestion: keep track of \"good builds\" by copy/pasting your choices into an empty cell and recording performance\n",
    "arch = \n",
    "G = \n",
    "W,B,costs,grad_norms = fit(X,Y,arch,G,alpha = 1e-3, \n",
    "                                momentum = 0,batch_size=300,lambda_=1,max_iters=1000,cost_type='se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to keep track of well-performing models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the costs (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gradient norms (no input needed)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37710466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see how well your model fits the dataset (no input needed)\n",
    "plot_fit(X,Y,W,B,G,size_ = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bfbe81",
   "metadata": {},
   "source": [
    "Plot the hidden features in the second-to-last layer (should be a layer of size two) versus the corresponding targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c806d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a function which plots the hidden features in the second-to-last layer (no input needed)\n",
    "def plot_hidden_features(X,Y,W,B,G,size_ = 2):\n",
    "    D = len(G) - 1\n",
    "    m = len(X)\n",
    "    X_feeds = {}\n",
    "    for i in range(m):\n",
    "        x=X[i,:]\n",
    "        feeds=feedforward(W,B,G,x)\n",
    "        X_feeds[i] = feeds\n",
    "    XH = np.array([X_feeds[i][D-1][0] for i in range(m)])\n",
    "    plot_figure = go.Figure(data=[go.Scatter3d(x=XH[:,0], y=XH[:,1], z=[r[0] for r in Y], mode='markers',marker=dict(size=size_))])\n",
    "    plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf843dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the last layer is linear (regression problem) the hidden features should approximately lie on a plane\n",
    "plot_hidden_features(X,Y,W,B,G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918313a",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "In the context of the previous part, explain how the neural network fits the data when the last layer is linear.\n",
    "\n",
    "Your explanation goes here (double click into the markdown cell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24663ee1",
   "metadata": {},
   "source": [
    "## Problem 5 Continued\n",
    "\n",
    "In the following cells, build a neural network without using the `Squared` activation and see how well you can fit this data. Use the `plot_fit` function to visualize the way your model has fit the data. \n",
    "\n",
    "Hint: you will have to try many architectures and hyperparameter values. Keep track of those which worked well so if you change and decrease performance, you know how to get back to a decent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2925b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to keep track of well-performing models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6db6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose your architecture, activations, and hyperparameters\n",
    "arch = \n",
    "G = \n",
    "W,B,costs,grad_norms = fit(X,Y,arch,G,alpha = 1e-3, \n",
    "                                momentum = .4,batch_size=500,lambda_=.1,max_iters=1500,cost_type='se')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345116af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the costs (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4832bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the norms of gradients (no input needed)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well your model fits the dataset\n",
    "plot_fit(X,Y,W,B,G,size_ = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc3870",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "\n",
    "The point of this problem is to use a neural network as a multiclass classifier. We will load the iris dataset. Each data point is a list of measurements of sepals and petals of a given iris plant. The targets correspond to the different species of iris plants. There are 3 different species. There are 4 measurements in each data point. \n",
    "\n",
    "To start, you must define a function `predict(W,B,G,x)` which takes as an input a data point `x` (i.e. a 4-dimensional vector) and the output is a one-hot encoding of the prediction your model gives i.e. either `np.array([1,0,0])`, `np.array([0,1,0])`, or `np.array([0,0,1])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c06e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction\n",
    "def predict(W,B,G,x):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99158daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabs a toy dataset from sklearn datasets (no input needed)\n",
    "from sklearn.datasets import load_iris,load_digits\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = np.zeros((data.target.shape[0],3))\n",
    "for i in range(data.target.shape[0]):\n",
    "    Y[i] = np.zeros(3)\n",
    "    Y[i][data.target[i]]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81c9fd",
   "metadata": {},
   "source": [
    "The following cell creates a test set and validation set to test hyperparameters. This can be done quickly using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b17063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation and test set (no input needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=4) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff219907",
   "metadata": {},
   "source": [
    "Now it is time to choose an architecture to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee574a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose an architecture, appropriate activation functions, and hyperparameters and cost type\n",
    "arch = \n",
    "G = \n",
    "W,B,costs,grad_norms = fit(X_train,y_train,arch,G,alpha = 1e-1,\n",
    "                                momentum = .1,batch_size=300,lambda_=0.1,max_iters=500, cost_type = 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the costs (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb290c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gradient norms (no input needed)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e39599",
   "metadata": {},
   "source": [
    "## Measuring performance\n",
    "\n",
    "Next, write code to compute the accuracy (number of correct predictions divided by number of examples) in the training set, validation set, and test set. Recall to get the best possible model, you should optimize the accuracy on the validation set. The accuracy on the test set is only computed after doing this to get a realistic expectation of your generalization error. Note: on such a small dataset there may be a lot of variance in these figures (specifically in validation and test set accuracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute training accuracy (write code to compute the accuracy of predictions on the training set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96eef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute validation accuracy (write code to compute the accuracy of predictions on the validation set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a89e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test accuracy (write code to compute the accuracy of predictions on the test set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8dbaa",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "\n",
    "The purpose of this problem is to predict handwritten digits with a neural network and compare performance to the naive Bayes model in the previous assignment.\n",
    "\n",
    "Our dataset will be the same as the one used in the previous assignment. Start by writing a function `predict(W,B,G,x,output_type = 'vector')` which takes a typical row of the dataset (flattened vector of intensities) and gives the model prediction as an output. Note we will use a neural network with Softmax output layer so predictions are made by finding the argument of the maximum entry of the output. Feed the input through the model and return a one-hot encoding of the prediction in the case that `output_type == 'vector'` or the digit prediction as an integer if `output_type == 'number'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction\n",
    "def predict(W,B,G,x,output_type = 'vector'):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb81806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and convert to one-hot encoding (no input needed)\n",
    "digit_data = load_digits()\n",
    "X = digit_data.data\n",
    "Y = np.zeros((digit_data.target.shape[0],10))\n",
    "for i in range(digit_data.target.shape[0]):\n",
    "    Y[i,:][digit_data.target[i]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e96bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into a training set, validation set, and test set (no input needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d77b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this cell to keep track of best performing models (no input needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7dc56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose your architecture, activations, and hyperparameters and cost type\n",
    "arch = \n",
    "G = \n",
    "W,B,costs,grad_norms = fit(X_train,y_train,arch,G,alpha = .01, \n",
    "                                momentum = .1,batch_size=100,lambda_=.4,max_iters=500,cost_type = 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9390562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the costs (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13717548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the norms of gradients (no input needed)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27db6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute training accuracy (write code to compute the accuracy of predictions on the training set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa65f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute validation accuracy (write code to compute the accuracy of predictions on the validation set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test accuracy (write code to compute the accuracy of predictions on the test set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cea3e2",
   "metadata": {},
   "source": [
    "## Check your own handwritten digits\n",
    "\n",
    "Get a piece of paper and a pen. Write down four digits of your choice, preferably legibly. Take pictures of each digit, and crop them to nearly square and tightly centered windows around the digit. Move the image files (preferably as `.jpg` files) to the same folder on your computer which contains this notebook. Save them as `a.jpg`, `b.jpg`, `c.jpg`, `d.jpg`. Use the following to check how the model classifies them. Make sure to change `r` to the optimal value found in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85205e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running, make sure you have files a.jpg, b.jpg, c.jpg, d.jpg in the folder containing this notebook\n",
    "# check: you should see the converted images of your files plotted below\n",
    "from PIL import Image, ImageEnhance # importing some packages to handle images\n",
    "from matplotlib import image\n",
    "digit_images, digit_arrays = {}, {}\n",
    "for i in ['a','b','c','d']: #for each i this converts the image to an 8 by 8 matrix with values from 0 to 16\n",
    "    digit_image = Image.open(f\"{i}.jpg\") # opening your image\n",
    "    digit_image = digit_image.resize((8,8)) # resize to 8 by 8 pixels\n",
    "    digit_image = ImageEnhance.Contrast(digit_image).enhance(10).convert('LA') # preprocess (contrast & grayscale)\n",
    "    digit_images[i] = digit_image\n",
    "    digit_arr = np.asarray(digit_images[i])[:,:,0] # create as 3-tensor but only need 0 slice matrix\n",
    "    digit_arr = (255 - digit_arr) # had opposite grayscale convention, need to correct it\n",
    "    digit_arr = digit_arr/255*16 # had entries up to 255 but we only want it up to 16 \n",
    "    digit_arr = np.rint(digit_arr-3) # rounding to integer values\n",
    "    digit_arr = digit_arr.clip(min = 0) # attempting to get rid of background darkness\n",
    "    digit_arrays[i] = digit_arr\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 10))\n",
    "for ax, image, label in zip(axes, digit_images.values(), digit_images.keys()):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"{label}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check: seeing if your model reads the digits correctly\n",
    "for i in ['a','b','c','d']:\n",
    "    x = np.reshape(digit_arrays[i],64)\n",
    "    prediction = predict(W,B,G,x,output_type = 'number')\n",
    "    print(f'predicted digit for {i}.jpg with Neural Network: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628b3c8",
   "metadata": {},
   "source": [
    "## Explain...\n",
    "\n",
    "In the Previous assignment, we never computed the accuracy of the naive Bayes model. Go back now and compute the training accuracy (the number of correct predictions divided by the number of examples). It takes awhile to compute, so it suffices to just use the first 200 training examples. You can modify the code used to compute training accuracy above. What is the accuracy of the naive Bayes model? Does the neural network model outperform the naive Bayes model?\n",
    "\n",
    "Your answer goes here... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10439a2e",
   "metadata": {},
   "source": [
    "# Problem 8\n",
    "\n",
    "See if you can handle a nonlinear binary classification task. You will construct a neural network whose output is a probability (number between 0 and 1). We will not bother with a validation set or test set but instead just try and see if we can fit the training set.\n",
    "\n",
    "Start by defining a prediction function, with a parameter `output_type` such that when when `output_type` is set to `'number'`, the output is `0` if the probability (the last feedforward) is less than `.5` and `1` otherwise. When `output_type` is set to `probability` then the output should be just the probability (the last feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction\n",
    "def predict(W,B,G,x,output_type = 'number'):\n",
    "    ######################### your code goes here ########################\n",
    "    feeds = feedforward(W,B,G,x)\n",
    "    Nx = feeds[-1][0]\n",
    "    if output_type == 'probability':\n",
    "        return Nx[0]\n",
    "    if Nx[0]<.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating and plotting a binary classification dataset to test our neural network (no input needed)\n",
    "points = np.random.normal(0, 50, size=(10000,3))\n",
    "y,c = [],[]\n",
    "for point in points:\n",
    "    if point[0]**2+point[1]**2+point[2]**2<60**2:\n",
    "        y.append([1])\n",
    "        c.append('red')\n",
    "    else:\n",
    "        y.append([0])\n",
    "        c.append('blue')\n",
    "X,y = np.array(points), np.array(y)\n",
    "# visualize the dataset \n",
    "trace = go.Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers',marker=dict(size = 1,color=c))\n",
    "plot_figure = go.Figure(data = [trace])\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0fe5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose your architecture, activations, and hyperparameters\n",
    "arch = \n",
    "G = \n",
    "W,B,costs,grad_norms = fit(X,y,arch,G,alpha = .001, momentum = 0.1,\n",
    "                           batch_size=200,lambda_=0,max_iters=1000,cost_type = 'se')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the costs (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the norms of gradients (no input needed)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute training accuracy\n",
    "acc = 0\n",
    "m = len(X)\n",
    "for i in range(len(X)):\n",
    "    if predict(W,B,G,X[i,:])==y[i][0]:\n",
    "        acc+=1/m\n",
    "print(f'the model has accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c7087",
   "metadata": {},
   "source": [
    "Let us plot the decision boundary. This is done by plotting all of the points with whose predictions are in a small neighborhood of `.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no input needed\n",
    "# green points indicate the decision boundary (points near .5 probability)\n",
    "# may have to zoom in to get past blue points to see the decision boundary\n",
    "red_points = []\n",
    "blue_points = []\n",
    "boundary_points = []\n",
    "for i in range(len(X)):\n",
    "    point = X[i,:]\n",
    "    prob = predict(W,B,G,X[i,:],output_type='probability')\n",
    "    if abs(prob-.5)<.1:\n",
    "        boundary_points.append(point)\n",
    "    else:\n",
    "        if point[0]**2+point[1]**2+point[2]**2<60**2:\n",
    "            red_points.append(point)\n",
    "        else:\n",
    "            blue_points.append(point)\n",
    "X_blue,X_red,X_boundary = np.array(blue_points),np.array(red_points),np.array(boundary_points)\n",
    "# visualize the dataset \n",
    "trace_red = go.Scatter3d(x=X_red[:,0], y=X_red[:,1], z=X_red[:,2], mode='markers',marker=dict(size = 2,color='red'))\n",
    "trace_blue = go.Scatter3d(x=X_blue[:,0], y=X_blue[:,1], z=X_blue[:,2], mode='markers',marker=dict(size = 1,color='blue'))\n",
    "trace_boundary = go.Scatter3d(x=X_boundary[:,0], y=X_boundary[:,1], z=X_boundary[:,2], mode='markers',marker=dict(size = 6,color='green'))\n",
    "plot_figure = go.Figure(data = [trace_red,trace_blue,trace_boundary])\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdb68a",
   "metadata": {},
   "source": [
    "# Problem 9\n",
    "\n",
    "Lets see how our built-from-scratch neural net performs on a larger dataset. We will load the MNIST handwritten digits dataset. We are using the `openml` module to fetch this dataset. Try to attain an accuracy on the test set of at least 93%. It should be possible to get 95% or higher with enough tuning. \n",
    "\n",
    "Start by defining a prediction function based on a softmax output activation. The output should be an integer (`0` through `9`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction\n",
    "def predict(W,B,G,x):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no input needed\n",
    "# import dataset (may take a moment)\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data.to_numpy()\n",
    "y = mnist.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no input needed\n",
    "# split data into training set, validation set, and test set, and create one-hot encoding of targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "# one-hot encoding training set\n",
    "Yt = np.zeros((len(y_train),10))\n",
    "for i in range(len(y_train)):\n",
    "    Yt[i,:][int(y_train[i])]=1\n",
    "# set length of validation set\n",
    "m_val = len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to keep track of best hyperparameter choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4df426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose your architecture, activations, and hyperparameters\n",
    "arch = \n",
    "G = \n",
    "W,B,costs,grad_norms = fit(X_train,Yt,arch,G,alpha = .3, momentum = .1,\n",
    "                     batch_size=100,lambda_=.001,max_iters=10000,cost_type = 'se')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot your costs (no input needed)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot your gradient norms (no input needed)\n",
    "plot_grads(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f043308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute validation accuracy (write code to compute validation set accuracy of current model)\n",
    "acc_val = \n",
    "print(f'the accuracy on the validation set is: {acc_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4656a",
   "metadata": {},
   "source": [
    "The following cells gives a technique to iteratively build models to find a high performing set of parameters. Each time you find a model which performs better than the existing best performing model, it will save over the weight values, bias values, architecture, and activations to JSON files \"weights.json\", \"biases.json\", \"arch.json\", \"G.json\". We turn `print_costs` to `False` to avoid a huge number of outputs. This will print the validation set accuracy every time we reach `max_iters` iterations, and indicate if this is better than the best accuracy seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this variable to keep track of the highest validation set accuracy you've seen so far (acc_val_highest)\n",
    "# careful... if you come back and rerun this cell it sets the variable back to zero, \n",
    "# then your best model may get replaced if you run the following cell afterwards.\n",
    "# if you restart the notebook, you may want to manually set this to the highest accuracy you've found so far\n",
    "acc_val_highest = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f447d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: You have to manually stop this cell from running otherwise it will run forever\n",
    "# Copy your best architecture, activation, and hyperparameter values below\n",
    "# repetitively run model on your best architecture found above. (no input needed)\n",
    "# try letting this run for an hour minimum\n",
    "# think of this as taking several SGD walks using your best architecture, stopping after a certain number\n",
    "# of iterations and then restarting. You get a sampling of good parameter values, and each time you \n",
    "# get the best performance you've seen so far, we save the parameter values to a JSON file\n",
    "# after running this for a long time, hopefully you'll find a model that performs very well.\n",
    "WT,BT = [],[]\n",
    "while True:\n",
    "    ######### choose your architecture, activations, and hyperparameters ###########\n",
    "    arch = [784,700,700,10]\n",
    "    G = [Linear,ReLU,ReLU,Softmax,]\n",
    "    W,B,costs,grad_norms = fit(X_train,Yt,arch,G,alpha = 3, momentum = .2,\n",
    "                         batch_size=20,lambda_=.000001,max_iters=3000,cost_type = 'ce',print_costs = False)\n",
    "    ################################################################################\n",
    "    # compute validation accuracy\n",
    "    acc_val = len([i for i in range(m_val) if predict(W,B,G,X_val[i,:])==int(y_val[i])])/m_val\n",
    "    print(f'the accuracy on the validation set is: {acc_val}')\n",
    "    if acc_val>acc_val_highest:\n",
    "        acc_val_highest = acc_val\n",
    "        WT,BT = W,B\n",
    "        print(f'the new highest accuracy on the validation set: {acc_val}')\n",
    "        jsonFile = open(\"weights.json\", \"w+\")\n",
    "        jsonFile.seek(0) # absolute file positioning\n",
    "        jsonFile.truncate() # to erase all data \n",
    "        jsonFile.close()\n",
    "        jsonFile = open(\"biases.json\", \"w+\")\n",
    "        jsonFile.seek(0) # absolute file positioning\n",
    "        jsonFile.truncate() # to erase all data \n",
    "        jsonFile.close()\n",
    "        jsonFile = open(\"arch.json\", \"w+\")\n",
    "        jsonFile.seek(0) # absolute file positioning\n",
    "        jsonFile.truncate() # to erase all data \n",
    "        jsonFile.close()\n",
    "        jsonFile = open(\"G.json\", \"w+\")\n",
    "        jsonFile.seek(0) # absolute file positioning\n",
    "        jsonFile.truncate() # to erase all data \n",
    "        jsonFile.close()\n",
    "        print('deleted old weights from weights.json and biases from biases.json')\n",
    "        # Serialization\n",
    "        Ws,Bs = {l:W[l] for l in range(len(W))},{l:B[l] for l in range(len(B))}\n",
    "        archs,Gs = {l:arch[l] for l in range(len(arch))},{l:str(G[l]) for l in range(len(G))}\n",
    "        print(\"serialized W,B,arch,G into JSON and saved to JSON files weights.json, biases.json, arch.json and G.json\")\n",
    "        with open(\"weights.json\", \"w\") as write_file:\n",
    "            json.dump(Ws, write_file, cls=NumpyArrayEncoder)\n",
    "        with open(\"biases.json\", \"w\") as write_file:\n",
    "            json.dump(Bs, write_file, cls=NumpyArrayEncoder)\n",
    "        with open(\"arch.json\", \"w\") as write_file:\n",
    "            json.dump(archs, write_file, cls=NumpyArrayEncoder)\n",
    "        with open(\"G.json\", \"w\") as write_file:\n",
    "            json.dump(Gs, write_file, cls=NumpyArrayEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d21cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fetch saved weights from local json file\n",
    "fileObject = open(\"weights.json\", \"r\")\n",
    "jsonContent = fileObject.read()\n",
    "Ws = json.loads(jsonContent)\n",
    "\n",
    "fileObject = open(\"biases.json\", \"r\")\n",
    "jsonContent = fileObject.read()\n",
    "Bs = json.loads(jsonContent)\n",
    "\n",
    "# to fetch saved activations and architecture from local json file\n",
    "fileObject = open(\"arch.json\", \"r\")\n",
    "jsonContent = fileObject.read()\n",
    "archs = json.loads(jsonContent)\n",
    "\n",
    "fileObject = open(\"G.json\", \"r\")\n",
    "jsonContent = fileObject.read()\n",
    "Gs = json.loads(jsonContent)\n",
    "\n",
    "# setting fetched parameters as our model parameters\n",
    "W = [np.array(Ws[i]) for i in Ws.keys()]\n",
    "B = [np.array(Bs[i]) for i in Bs.keys()]\n",
    "arch = [archs[i] for i in archs.keys()]\n",
    "G = [eval(Gs[i][10:][:-16]) for i in archs.keys()]\n",
    "\n",
    "print(f'current best architecture: {arch}')\n",
    "print(f'current best activation structure: {G}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run test accuracy after optimizing validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test accuracy\n",
    "acc_test = 0\n",
    "m_test = len(X_test)\n",
    "for i in range(len(X_test)):\n",
    "    if predict(W,B,G,X_test[i,:])==int(y_test[i]):\n",
    "        acc_test+=1/m_test\n",
    "acc_test\n",
    "print(f'the accuracy on the test set is: {acc_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
