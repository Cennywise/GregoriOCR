{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034abf95",
   "metadata": {},
   "source": [
    "# <center> Computational Homework 3</center>\n",
    "\n",
    "In this notebook we look at some computational problems in probability theory and optimization. Throughout the notebook, vectors and matrices should always be represented as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48add7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import needed modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import random, time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9e5e7",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In class we proved that if $J$ is twice differentiable, convex, and has $L$ - Lipshchitz continuous gradient, then for fixed step size $\\alpha\\leq \\frac{1}{L}$ we have $$f(x_k)-f(x^*)\\leq \\frac{||x_0-x^*||_2^2}{2\\alpha k}.$$\n",
    "\n",
    "The goal of this problem is to verify this for linear regression. We will let gradient descent run for longer than one would ever need to in practice, to see how long it takes to reach the desired accuracy in approximating the global minimum. Copy the code from Homework 2 Problem 3 to complete the cell below. Now change the code as follows. First, we only want to print the cost once every 10000 iterations. \n",
    "\n",
    "We learned in class how to solve for the global minimum explicitly using \n",
    "\n",
    "$$v^* = (\\hat{\\mathbf{X}}^\\top\\hat{\\mathbf{X}})^{-1}\\hat{\\mathbf{X}}^\\top y$$\n",
    "\n",
    "Change the stopping condition so that you stop once the cost is within a distance `epsilon` from the cost of the global minimum $v^*$. You can use `LA.inv` to compute the inverse in your computation of $v^*$. \n",
    "\n",
    "Run the two cells below (the 2nd defines a randomized dataset to test on). \n",
    "\n",
    "The markdown cell after provides additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aed056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to complete problem 1, fill in the following with your code\n",
    "def J(X,y,v):\n",
    "    ######################### your code goes here ########################\n",
    "    \n",
    "def DJ(X,y,v):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "def GD_linreg(X,y,alpha,epsilon,max_iters = 10000): \n",
    "    ######################### your code goes here ########################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dataset to test the model (no input needed)\n",
    "X = np.random.normal(0, 500, size=(4000,2))\n",
    "y = np.array([[np.random.normal(3,2)*point[0]+np.random.normal(4,2)*point[1]+np.random.normal(5,5)] for point in X])\n",
    "Xhat = np.concatenate((np.ones((len(X),1)),X), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780894e",
   "metadata": {},
   "source": [
    "## Problem 1 Continued...\n",
    "\n",
    "In theoretical homework 2 you found the $L$ for which $\\nabla J$ is $L$-Lipschitz continuous. Compute this value of $L$ and save it to the variable `L`. You may use `LA.norm(MATRIX,2)` to compute the 2-norm which comes up in the formula. Compute the global minimum parameters $v^*$ and save it to a variable `global_min`, and the global minimum value $J(\\mathbf{X},\\mathbf{y},\\mathbf{v}^*)$ of the cost function and set it to a variable `gmcost`. Set `alpha` to be equal to $\\frac{1}{L}$. Set the value of `epsilon` to be the global minimum cost times `1e-7`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf4f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute L here (input needed)\n",
    "L = \n",
    "print(f'the gradient of J is L-Lipschitz continuous for L = {L}')\n",
    "\n",
    "# compute the global minimum cost here (input needed)\n",
    "global_min = \n",
    "gmcost = \n",
    "print(f'the global minimum cost is {gmcost}')\n",
    "\n",
    "# Tuning parameters (input needed for alpha, epsilon)\n",
    "alpha = \n",
    "print(f'the value of the step size alpha is {alpha}')\n",
    "epsilon = \n",
    "print(f'the value of epsilon is {epsilon}')\n",
    "max_iters = 2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23da5a",
   "metadata": {},
   "source": [
    "## Problem 1 Continued...\n",
    "\n",
    "Then run the following cell to run gradient descent until desired convergence (it will take awhile... maybe three or four minutes). If this does not converge after 2 million iterations, you can the stopping condition parameter set previously from `1e-7` to `1e-6`. Run gradient descent and note the number of steps taken until stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d583e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the gradient descent algorithm (no input needed)\n",
    "v,costs = GD_linreg(X,y,alpha,epsilon,max_iters)\n",
    "print('Using gradient descent, we have found the approximate global minimum:')\n",
    "print(v)\n",
    "# print the actual global minimum parameters \n",
    "print('Solving analytically, we know the actual global minimum is at:')\n",
    "print(global_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b7bb6",
   "metadata": {},
   "source": [
    "## Problem 1 Continued...\n",
    "\n",
    "In this problem we are stopping gradient descent when $f(x_k)-f(x^*)<\\epsilon$. Using the fact that $$f(x_k)-f(x^*)\\leq \\frac{||x_0-x^*||_2^2}{2\\alpha k}$$ compute the number of steps `k` needed until we are guaranteed to stop. Compute this in the code cell below and then explain in the markdown cell below whether this makes sense in the context of the previous step of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the quantity k here (input needed)\n",
    "k = \n",
    "print(f'The maximum number of steps needed for convergence is {int(np.ceil(k))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6c083",
   "metadata": {},
   "source": [
    "## Explain the result here\n",
    "\n",
    "Replace this text with your explanation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ed6e0",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "The goal of this problem is to implement regularized linear regression with polynomial terms up to degree `k`. Start by defining a function `generate_monomials_eq(n,k)` with inputs positive integers `n`,`k`, which outputs a list of all lists of length `n` whose entries are nonnegative integers that sum to `k`. Return this list sorted lexicographically bigger to smaller (the `sorted()` function can handle this with the argument `reverse = True`). I suggest doing this recursively (i.e. inside of the function `generate_monomials_eq(n,k)` call the function `generate_monomials_eq(n,k-1)` and then for each monomial in this list, add one to each component, giving you `n` new monomials of degree `k`. Add each of these to the list of monomials of degree `k` (unless it is already in the list). You'll need to program the base case where $k=0$ by hand.\n",
    "\n",
    "Also define a function `generate_monomials_leq(n,k)` which gives the list of all lists of length `n` whose entries add up to a number between `0` and `k`. You should create this list by putting together results from `generate_monomials_eq(n,i)` for $0\\leq i\\leq k$.\n",
    "\n",
    "Note: if you have a better way to implement both in a single function `generate_monomials_leq` you can do that as well, as long as the result is correct.\n",
    "\n",
    "Running the following cell will test that you are getting the correct number of polynomial terms in some special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to complete problem 2, fill in the following with your code\n",
    "\n",
    "def generate_monomials_eq(n,k):\n",
    "    ######################### your code goes here ########################\n",
    "    \n",
    "def generate_monomials_leq(n,k):\n",
    "    ######################### your code goes here ########################\n",
    "    \n",
    "# testing your functions\n",
    "print(len(generate_monomials_leq(2,4))) #should be 15\n",
    "print(len(generate_monomials_leq(3,6))) #should be 84\n",
    "print(len(generate_monomials_leq(7,4))) #should be 330\n",
    "print(len(generate_monomials_leq(9,9))) #should be 48620 (should take about 25 seconds to compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322e128",
   "metadata": {},
   "source": [
    "## Problem 2 Continued...\n",
    "\n",
    "Define a function `add_poly_terms(X,k)` which adds columns to the dataset `X` corresponding to all polynomial terms up to total degree `k` (e.g. if `n==2` and `X` has two columns, you would have columns corresponding to $1,x_1,x_2,x_1^2, x_1x_2, x_2^2$). Hint: you can make a for loop over all of the monomials in `mons = generate_monomials_leq(n,k)` (i.e. `for mon in mons:`) and a for loop over the number of examples. You want to create matrix entries of the form `X[i,1]**mon[1] * X[i,2]**mon[2] * ... * X[i,n]**mon[n]` which can be accomplished with another loop.\n",
    "\n",
    "Note: You do not need to concatenate columns to `X`. Instead, create a new array with a row for each polynomial term in `generate_monomials_leq(n,k)`. Since your `generate_monomials_leq` has terms corresponding to $1,x_1,x_2,\\dots,x_n$, this will contain the old dataset, plus a bias term, plus all of the other higher order polynomial terms. Since you added these as rows (this is more natural in numpy) at the end you should return the transpose of the numpy array you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_poly_terms(X,k):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09865fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset and testing that poly terms are added correctly\n",
    "X = np.array([[2,2],[0,1],[1,1],[2,4]])\n",
    "Xhat = add_poly_terms(X,2)\n",
    "print('the original matrix X:')\n",
    "print(X)\n",
    "print('the matrix X with polynomial terms up to degree 2')\n",
    "print(Xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef745365",
   "metadata": {},
   "source": [
    "## Problem 2 Continued...\n",
    "\n",
    "Copy your code from Computational Homework 2 Problem 3 (just `J` and `DJ`) and Homework 2 Problem 4 (use the improved gradient descent using the Hessian) into the cell below. Make the following changes: \n",
    "\n",
    " - get rid of portion of code that concatenates a column of 1's on the left (our `add_poly_terms` function includes the bias term automatically)\n",
    " - add an argument `lambda_` to `J` (at the end), `DJ` (at the end) and `GD_linreg_improved` (before `max_iters`)\n",
    " - add a regularization term\n",
    " - modify the `DJ` function accordingly to account for the regularization term\n",
    " - modify the Hessian to account for the regularization term\n",
    " - print the cost every 1000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(X,y,v,lambda_):\n",
    "    ######################### your code goes here ########################\n",
    "    \n",
    "def DJ(X,y,v,lambda_):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "def GD_linreg_improved(X,y,epsilon,lambda_,max_iters = 10000): \n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dataset to test the model (no input needed)\n",
    "points = []\n",
    "points_val = []\n",
    "points_test = []\n",
    "for i in range(-3,3):\n",
    "    for j in range(-3,3):\n",
    "        points.append([i+np.random.normal(0,.5),j+np.random.normal(0,.5)])\n",
    "        points_val.append([i+np.random.normal(0,.5),j+np.random.normal(0,.5)])\n",
    "        points_test.append([i+np.random.normal(0,.5),j+np.random.normal(0,.5)])\n",
    "X = np.array(points)\n",
    "def p_data(p):\n",
    "    return 5*p[0]**2 - p[1]**3 + np.random.normal(0,2)\n",
    "y = np.array([[p_data(p)] for p in X])\n",
    "X_val = np.array(points_val)\n",
    "y_val = np.array([[p_data(p)] for p in X_val])\n",
    "X_test = np.array(points_test)\n",
    "y_test = np.array([[p_data(p)] for p in X_test])\n",
    "# plotting training set\n",
    "plot_figure = go.Figure(data=[go.Scatter3d(x=X[:,0], y=X[:,1], z=[r[0] for r in y], mode='markers',marker=dict(size=5))])\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0daf0a",
   "metadata": {},
   "source": [
    "## Problem 2 Continued...\n",
    "\n",
    "Next we run our gradient descent algorithm. First run the following 5 cells with `lambda_ = 0` and observe how the model performs. Adjust the value of `lambda_` until you find a good fit (not overfit, not underfit). This can be determined by comparing training set performance, validation set performance, and test set performance by following the procedure explained in lecture. Explain in the markdown cell at the end the relationship between the values of `degree` and of `lambda_` and whether the model overfits or underfits. Things to consider: \n",
    "\n",
    " - Experiment with different values of `lambda_` with the given `degree`. \n",
    " - Now experiment with the `degree`. \n",
    " - Can you find an optimal choice of both? \n",
    " - Does the effect of `lambda` change based on your choice of `degree`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de3cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this by default fits a model without regularization (lambda_ = 0)\n",
    "# INPUT NEEDED\n",
    "# pick appropriate values for degree and lambda_ (can also change epsilon if needed)\n",
    "epsilon = .01\n",
    "degree = 10\n",
    "lambda_ = 0\n",
    "Xhat = add_poly_terms(X,degree)\n",
    "print(f'number of features in Xhat: {Xhat.shape[1]}')\n",
    "v,costs = GD_linreg_improved(Xhat,y,epsilon,lambda_)\n",
    "print(f'unregularized cost on training set: {J(Xhat,y,v,0)}')\n",
    "# Computing the validation set error (no input needed)\n",
    "Xhat_val = add_poly_terms(X_val,degree)\n",
    "print(f'unregularized cost on validation set: {J(Xhat_val,y_val,v,0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the test set error (no input needed)\n",
    "Xhat_test = add_poly_terms(X_test,degree)\n",
    "print(f'unregularized cost on test set: {J(Xhat_test,y_test,v,0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well your result fits the training set (no input needed)\n",
    "zz = [r[0] for r in y]\n",
    "trace = go.Scatter3d(x=Xhat[:,1], y=Xhat[:,2], z=zz, mode='markers',marker=dict(size=5))\n",
    "xs,ys = Xhat[:,1],Xhat[:,2]\n",
    "xxx = np.outer(np.linspace(min(xs), max(xs), 30), np.ones(30))\n",
    "yyy = np.outer(np.linspace(min(ys), max(ys), 30), np.ones(30)).T\n",
    "zzz = 0\n",
    "mons = generate_monomials_leq(X.shape[1],degree)\n",
    "i=0\n",
    "for mon in mons:\n",
    "    zzz +=  v[i]*xxx**(mon[0])*yyy**(mon[1])\n",
    "    i+=1\n",
    "\n",
    "# Configure the layout.\n",
    "layout = go.Layout(margin={'l': 0, 'r': 0, 'b': 0, 't': 0})\n",
    "data = [trace,go.Surface(x=xxx, y=yyy, z=zzz, showscale=False, opacity=0.5)]\n",
    "# Render the plot.\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "plot_figure.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis = dict(nticks=4, range=[-4,4],),\n",
    "                     yaxis = dict(nticks=4, range=[-4,4],),\n",
    "                     zaxis = dict(nticks=4, range=[min(zz),max(zz)],),),\n",
    "    width=700,\n",
    "    margin=dict(r=20, l=10, b=10, t=10))\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well your result fits the test set (no input needed)\n",
    "zz_test = [r[0] for r in y_test]\n",
    "trace = go.Scatter3d(x=Xhat_test[:,1], y=Xhat_test[:,2], z=zz_test, mode='markers',marker=dict(size=5))\n",
    "xs,ys = Xhat_test[:,1],Xhat_test[:,2]\n",
    "xxx = np.outer(np.linspace(min(xs), max(xs), 30), np.ones(30))\n",
    "yyy = np.outer(np.linspace(min(ys), max(ys), 30), np.ones(30)).T\n",
    "zzz = 0\n",
    "mons = generate_monomials_leq(X.shape[1],degree)\n",
    "i=0\n",
    "for mon in mons:\n",
    "    zzz +=  v[i]*xxx**(mon[0])*yyy**(mon[1])\n",
    "    i+=1\n",
    "# Configure the layout.\n",
    "layout = go.Layout(margin={'l': 0, 'r': 0, 'b': 0, 't': 0})\n",
    "data = [trace,go.Surface(x=xxx, y=yyy, z=zzz, showscale=False, opacity=0.5)]\n",
    "# Render the plot.\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "plot_figure.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis = dict(nticks=4, range=[-4,4],),\n",
    "                     yaxis = dict(nticks=4, range=[-4,4],),\n",
    "                     zaxis = dict(nticks=4, range=[min(zz_test),max(zz_test)],),),\n",
    "    width=700,\n",
    "    margin=dict(r=20, l=10, b=10, t=10))\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55be27",
   "metadata": {},
   "source": [
    "## Explain the result here\n",
    "\n",
    "Replace this text with your explanation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed9999",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In this problem our goal is to implement the Gaussian Naive Bayes Classification algorithm. We test it on handwritten digits. Recall, in naive Bayes, our goal is to find a maximum likelihood estimator for the following approximate conditional probability distribution:\n",
    "\n",
    "$p(y=j|\\vec{x}) = p(y=j)\\prod_{i=1}^n p(x_i|y=j)$\n",
    "\n",
    "Model $p(y)$ as a categorical distribution and $p(x_i|y=j)$ as a Gaussian as described in the lecture. Careful though, if the variance is computed to be 0, you can not use the Gaussian formula (you decide how to handle this). \n",
    "\n",
    "You are given a supervised learning dataset `X,y` where `X` is a 2-dimensional numpy array and `y` is a 1-dimensional numpy array. You will complete functions `p(j,X,y)` which models the distribution $p(y=j)$, a function `p_cond(x,i,j,X,y)` which models $p(x_i|y=j)$, and a function `bayes_prediction(x,X,y)` which returns the corresponding class prediction. The inputs are\n",
    "\n",
    "- `x` an example which we can run prediction on (like a row of `X`)\n",
    "- `i` a feature index (so `i` ranges from `0` to `X.shape[1]`)\n",
    "- `j` a target label (so `j` takes any possible value one sees in the dataset `y`)\n",
    "- `X` a matrix of examples\n",
    "- `y` a vector of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to complete problem 3, fill in the following with your code\n",
    "def p(j,X,y): \n",
    "    ######################### your code goes here ########################\n",
    "    \n",
    "def p_cond(x,i,j,X,y):\n",
    "    ######################### your code goes here ########################\n",
    "    \n",
    "def bayes_prediction(x,X,y):\n",
    "    ######################### your code goes here ########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7999dee",
   "metadata": {},
   "source": [
    "## Test your model on the handwritten digit recognition problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no input needed\n",
    "# run this cell to load images of digits \n",
    "# showing some examples of typical images in the dataset\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "# Defining the dataset \n",
    "X,y = digits.data, digits.target\n",
    "# Plotting the first 10 examples\n",
    "fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 10))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Label: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (no input needed)\n",
    "# using the bayes_prediction function to predict the digits\n",
    "# you should get most correct\n",
    "for i in range(20):\n",
    "    pred = bayes_prediction(X[i,:],X,y)\n",
    "    target = y[i]\n",
    "    print(f'predicting: {pred}, true value: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8cc4c",
   "metadata": {},
   "source": [
    "## Step 4: Check your own handwritten digits\n",
    "\n",
    "Get a piece of paper and a pen. Write down four digits of your choice, preferably legibly. Take pictures of each digit, and crop them to nearly square and tightly centered windows around the digit. Move the image files (preferably as `.jpg` files) to the same folder on your computer which contains this notebook. Save them as `a.jpg`, `b.jpg`, `c.jpg`, `d.jpg`. Use the following to check how the model classifies them. Make sure to change `r` to the optimal value found in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before running, make sure you have files a.jpg, b.jpg, c.jpg, d.jpg in the folder containing this notebook\n",
    "# check: you should see the converted images of your files plotted below\n",
    "from PIL import Image, ImageEnhance # importing some packages to handle images\n",
    "from matplotlib import image\n",
    "digit_images, digit_arrays = {}, {}\n",
    "for i in ['a','b','c','d']: #for each i this converts the image to an 8 by 8 matrix with values from 0 to 16\n",
    "    digit_image = Image.open(f\"{i}.jpg\") # opening your image\n",
    "    digit_image = digit_image.resize((8,8)) # resize to 8 by 8 pixels\n",
    "    digit_image = ImageEnhance.Contrast(digit_image).enhance(10).convert('LA') # preprocess (contrast & grayscale)\n",
    "    digit_images[i] = digit_image\n",
    "    digit_arr = np.asarray(digit_images[i])[:,:,0] # create as 3-tensor but only need 0 slice matrix\n",
    "    digit_arr = (255 - digit_arr) # had opposite grayscale convention, need to correct it\n",
    "    digit_arr = digit_arr/255*16 # had entries up to 255 but we only want it up to 16 \n",
    "    digit_arr = np.rint(digit_arr-3) # rounding to integer values\n",
    "    digit_arr = digit_arr.clip(min = 0) # attempting to get rid of background darkness\n",
    "    digit_arrays[i] = digit_arr\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 10))\n",
    "for ax, image, label in zip(axes, digit_images.values(), digit_images.keys()):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"{label}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (no input needed)\n",
    "# check: seeing if your model reads the digits correctly (3 out of 4 is ok)\n",
    "for i in ['a','b','c','d']:\n",
    "    x = np.reshape(digit_arrays[i],64)\n",
    "    prediction = bayes_prediction(x,X,y)\n",
    "    print(f'predicted digit for {i}.jpg with Naive Bayes: {prediction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
