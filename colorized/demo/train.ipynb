{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's load a dataset and try training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from scripts import forward_process as fp, reverse_process as rp, train, util\n",
    "from scripts import customUNet as un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab1 = np.load(\"../data/image-colorization/ab/ab/ab1.npy\")\n",
    "ab2 = np.load(\"../data/image-colorization/ab/ab/ab2.npy\")\n",
    "ab3 = np.load(\"../data/image-colorization/ab/ab/ab3.npy\")\n",
    "l = np.load(\"../data/image-colorization/l/gray_scale.npy\")\n",
    "\n",
    "ab = np.concatenate((ab1,ab2,ab3),axis=0)\n",
    "\n",
    "def createLab(n=100):\n",
    "    lab = np.zeros((n,224,224,3))\n",
    "    \n",
    "    lab[:,:,:,0] = l[0:n:]\n",
    "    lab[:,:,:,1:] = ab[0:n:]    \n",
    "    \n",
    "    return lab.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load 1000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "lab = createLab(1000)\n",
    "\n",
    "# Resize to 64x64 for lower computational load\n",
    "resized = []\n",
    "for i in lab:\n",
    "    img = cv2.normalize(i, None, alpha = -1, beta = 1, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "    resized.append(cv2.resize(img, dsize=(64, 64), interpolation=cv2.INTER_CUBIC))\n",
    "\n",
    "# Convert to tensor\n",
    "lab = torch.tensor(np.array(resized))\n",
    "dataloader = DataLoader(lab, batch_size=25, shuffle=True)\n",
    "print(len(lab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. Define custom DataLoader: \\\n",
    "    * \\__init__ \\\n",
    "    * \\__len__ \\\n",
    "    * \\__getitem__ \\\n",
    "\n",
    "See https://www.kaggle.com/code/varunnagpalspyz/pix2pix-is-all-you-need#%F0%9F%A7%B0-Utility-Functions \\\n",
    "\n",
    "2. Choose valid data\n",
    "\n",
    "3. Load training data, valid data\n",
    "\n",
    "4. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "results_folder = Path(\"../results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110075811\n"
     ]
    }
   ],
   "source": [
    "model = un.CustomConditionalUNet()\n",
    "device = util.set_device()\n",
    "model.to(device)\n",
    "x = sum(p.numel() for p in model.parameters())\n",
    "print(x)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m diffTerms \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mConstantDiffusionTerms(\u001b[38;5;241m1000\u001b[39m, fp\u001b[38;5;241m.\u001b[39mlinear_beta_schedule)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffTerms\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/MAT_180_ML_Projects/colorized/demo/../scripts/train.py:26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, device, optimizer, train_loader, batch_size, timesteps, constantDiffusionTerms, model)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step,(batch_imgs,batch_cond_imgs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     24\u001b[0m         \n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Reset the gradients to 0\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# Sample a timestep t from a uniform distribution for every example in the \u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# batch (long tensor)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,timesteps,(batch_size,),device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "diffTerms = util.ConstantDiffusionTerms(1000, fp.linear_beta_schedule)\n",
    "train.train(5,device,optimizer,dataloader,50,1000, diffTerms,model)\n",
    "\n",
    "# from torchvision.utils import save_image\n",
    "\n",
    "# epochs = 5\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for step, batch in enumerate(dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         batch_size = batch.shape[0]\n",
    "#         batch = batch.to(util.set_device())\n",
    "\n",
    "#       # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "#         t = torch.randint(0, 1000, (batch_size,), device=device).long()\n",
    "\n",
    "#         loss = train.p_loss(model, batch, t, loss_type=\"huber\")\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(\"Loss:\", loss.item())\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#       # save generated images\n",
    "#         if step != 0 and step % save_and_sample_every == 0:\n",
    "#             milestone = step // save_and_sample_every\n",
    "#             batches = num_to_groups(4, batch_size)\n",
    "#             all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
    "#             all_images = torch.cat(all_images_list, dim=0)\n",
    "#             all_images = (all_images + 1) * 0.5\n",
    "#             save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
