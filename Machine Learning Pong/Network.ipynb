{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Making the Neural Network\n",
    "\n",
    "This notebook contains explanations about the classes related to neural networks.\n",
    "\n",
    "The Neural Network consists of two classes, network and layer located in the Network directory in the Network.py file. Each of these classes will be discussed in detail in their own section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions\n",
    "\n",
    "The Network file has two helper functions, ReLU and Sigmoid which take in a vector and apply the function to each value in the vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    return [xi if xi > 0 else 0 for xi in x]\n",
    "\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return [0 if xi < 0 else 1 for xi in x]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Layer Class\n",
    "\n",
    "The layer class represents each layer in the Neural Network. It consists of an activation function, a matrix of weights, and a vector of biases.\n",
    "\n",
    "The layer class consists of two functions: activate and update.\n",
    "\n",
    "Activate takes in a vector of input values and outputs the layer's function applied to the product of the inputs and the layer's weights plus the layer's bias vector.\n",
    "\n",
    "Update takes in a mutation rate and a change value. It goes through each weight and bias, generating a random number for each. If this number is less than the mutation rate it adds a random number sampled from a uniform distribution that goes from (-change value, change value). This slightly modifies the network, possibly giving it better results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, bias, weights, function=\"ReLU\"):\n",
    "        self.function = function\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "\n",
    "    def activate(self, inputs):\n",
    "        if self.function == \"ReLU\":\n",
    "            return ReLU(np.dot(self.weights, inputs) + self.bias)\n",
    "        if self.function == \"Sigmoid\":\n",
    "            return Sigmoid(np.dot(self.weights, inputs) + self.bias)\n",
    "\n",
    "    def update(self, mutation_rate, change_value):\n",
    "        for i in range(len(self.weights)):\n",
    "            for j in range(len(self.weights[i])):\n",
    "                if np.random.random() < mutation_rate:\n",
    "                    self.weights[i][j] += np.random.uniform(-change_value, change_value)\n",
    "        for i in range(len(self.bias)):\n",
    "            if np.random.random() < mutation_rate:\n",
    "                self.bias[i] += np.random.uniform(-change_value, change_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Network Class\n",
    "\n",
    "The network class consists of a list of layer objects. It can accept weights and bias vectors if you want to replicate a network, or it will randomly generate the values as shown in class. The last layer will have Sigmoid activation while all the other layers will have ReLU activation.\n",
    "\n",
    "The network class has 3 functions: calc, update, and getWeights.\n",
    "\n",
    "The calc function takes an input vector and feeds it through the network, returning the output.\n",
    "\n",
    "The update function calls update on each layer of the network.\n",
    "\n",
    "The getWeights function returns the weights and biases of the network as a tuple."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Network:\n",
    "    # Input_size is the size of the initial input data\n",
    "    # Shape is an array of integers with each one containing the size of a layer for the network\n",
    "    # kwargs will contain the bias and weights if you want to pre-initialize the network from a previous run\n",
    "    def __init__(self, input_size, shape, **kwargs):\n",
    "\n",
    "        bias = kwargs.get(\"bias\", None)\n",
    "        weights = kwargs.get(\"weights\", None)\n",
    "        self.layers = []\n",
    "        previous_layer = input_size\n",
    "\n",
    "        # Run through each layer that needs to be created\n",
    "        for i in range(len(shape)):\n",
    "            layer = shape[i]\n",
    "\n",
    "            # Check if there are preset bias and weights, otherwise randomize them as shown in class\n",
    "            if bias is not None and weights is not None:\n",
    "                b = bias[i]\n",
    "                w = weights[i]\n",
    "            else:\n",
    "                b = np.zeros(layer)\n",
    "                w = np.random.default_rng().normal(loc=0, scale= 2 / (layer + previous_layer), size=(layer, previous_layer))\n",
    "\n",
    "            # If it is the last layer use \"Sigmoid\", otherwise use the default of ReLU\n",
    "            if i == len(shape) - 1:\n",
    "                self.layers.append(Layer(bias=b, weights=w, function=\"Sigmoid\"))\n",
    "            else:\n",
    "                self.layers.append(Layer(bias=b, weights=w))\n",
    "\n",
    "            previous_layer = layer\n",
    "\n",
    "    # Given the input, run through the network to get the output\n",
    "    def calc(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.activate(inputs)\n",
    "        return inputs\n",
    "\n",
    "    # Update the weights and bias in the network by some random amount\n",
    "    # Can accept a mutation_rate value which is the rate of change and a change_value which is the max amount of change\n",
    "    def update(self, mutation_rate, change_value):\n",
    "        for layer in self.layers:\n",
    "            layer.update(mutation_rate, change_value)\n",
    "\n",
    "    def getWeights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for layer in self.layers:\n",
    "            weights.append(layer.weights)\n",
    "            biases.append(layer.bias)\n",
    "        return weights, biases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the Network Class\n",
    "\n",
    "The network class takes a value called input_size, which is the size of the input the network will receive and an array called shape which is a list of the size of each layer. As optional parameters you can pass in a tensor of weights and a matrix of biases to initialize the network with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Try playing around with various network and input sizes\n",
    "network = Network(4, [8,7,6,1], bias=None, weights=None)\n",
    "inputs = [1, 2, 3, 4]\n",
    "\n",
    "output = network.calc(inputs)\n",
    "print(output)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
