{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import random, time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions----------------------------------------------------------------\n",
    "# INPUT: numpy array\n",
    "# OUTPUT: numpy array\n",
    "def ReLU(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "    out = []       \n",
    "    if deriv == True:\n",
    "        for xval in x:\n",
    "            if xval < 0:\n",
    "                out.append(0)\n",
    "            elif xval > 0:\n",
    "                out.append(1)\n",
    "    else:\n",
    "        for xval in x:\n",
    "            out.append(max(0,xval)) \n",
    "    return out\n",
    "    \n",
    "def Linear(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "    out = []\n",
    "    if deriv == True:\n",
    "        for xval in x:\n",
    "            out.append(1)   \n",
    "        return out\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def Sigmoid(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "    out = []\n",
    "    if deriv == True:\n",
    "        for xval in x:\n",
    "            out.append(1/(1+np.exp(-xval)))\n",
    "    else:\n",
    "        for xval in x:  \n",
    "            sigdev = 1/(1+np.exp(-xval)) * (1-1/(1+np.exp(-xval)))\n",
    "            out.append(sigdev)\n",
    "    return out\n",
    "\n",
    "def Squared(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "    if deriv == True:\n",
    "        return 2*x\n",
    "    else:\n",
    "        return x**2\n",
    "\n",
    "def Softmax(x,deriv = False):\n",
    "    ######################### your code goes here ########################\n",
    "    func = np.exp(x)/sum(np.exp(x))\n",
    "    if deriv == True:\n",
    "        return func-func@func.T\n",
    "    else:\n",
    "        return func\n",
    "#----------------------LOSS FUNCTION--------------------------------------------------------------------------\n",
    "\n",
    "# Note: Nx and y are always numpy arrays (for 'bce' they always have only one entry)\n",
    "# when deriv = False the output must be a number and when deriv = True the output must be a vector\n",
    "def loss(Nx,y,cost_type,deriv = False):\n",
    "    # square error------------------\n",
    "    if cost_type == 'se':\n",
    "        if deriv == True:\n",
    "            return 2*(Nx - y).T #vector\n",
    "        else: \n",
    "            argument = Nx-y\n",
    "            value = (np.linalg.norm(argument))**2 #number\n",
    "            return value \n",
    "    # cross entropy-----------------\n",
    "    elif cost_type == 'ce':\n",
    "        if deriv == True:\n",
    "            value = -y.T @ np.diag(1/Nx)\n",
    "            return value\n",
    "        else:\n",
    "            return -y.T @ np.log(Nx)\n",
    "    # binary cross entropy---------- \n",
    "    else:\n",
    "        if deriv == True:\n",
    "            return -y/Nx + (1-y)/(1+Nx)\n",
    "        else:\n",
    "            return -y@np.log(Nx)-(1-y)@np.log(1-Nx)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(W,B,G,x):\n",
    "    ######################### your code goes here ########################\n",
    "    # W has 5 matrices W[0] to W[4]\n",
    "    # B has 5 vectors\n",
    "    # G has 6 functions\n",
    "    # x has 10 entries\n",
    "     \n",
    "    # INITIAL SIGNAL\n",
    "    feeds = []\n",
    "    #-----Debug------------------------------\n",
    "    #print(W[0].shape, x.shape, B[0].shape)\n",
    "    #----------------------------------------\n",
    "    si = (W[0] @ x) + B[0]\n",
    "    \n",
    "    #feeds = [x0,si] # for index 0\n",
    "    feeds.append([x,si])\n",
    "    \n",
    "    for i in range(1,len(G)): # len(G) = 6 (stops at index 5)\n",
    "        xi = G[i](si) # FUNCTION CALL\n",
    "        if i != (len(G)-1):\n",
    "            si = (W[i] @ xi) + B[i] #stop before last index\n",
    "            feeds.append([xi,si])\n",
    "        else:\n",
    "            feeds.append(xi)\n",
    "    return feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas(X_feeds,Y,W,B,G,verbose = False, cost_type = 'se'):\n",
    "    ######################### your code goes here ########################\n",
    "    # TASK: create dictionary with {keys: deltas}\n",
    "    \n",
    "    #note: X_feeds.keys() = [0,1]\n",
    "    \n",
    "    # initialize layers l\n",
    "    D = len(G)-1 # (D = 5 for first test)\n",
    "    # initialize deltas dictionary: deltas_dict = {0:[delta^(0),delta^(l),delta^(D-1)] , 1:[delta^(0),delta^(l),delta^(D-1)]\n",
    "    deltas_dict = {}\n",
    "    keys = X_feeds.keys()\n",
    "    [deltas_dict.setdefault(i,[np.zeros(len(B[l]) ) for l in range(D)]) for i in keys] \n",
    "    #X_feeds contains lots of nan arrays after a few iterations in problem 4\n",
    "\n",
    "    #loop for each key in dictionary X_feeds to create ***deltas_dict***\n",
    "    for i in X_feeds.keys():\n",
    "        \n",
    "        #first delta^(D-1)\n",
    "        Nx = X_feeds[i][D][0] #********Nx = last x from X_feeds********\n",
    "        s0 = X_feeds[i][D-1][1] #D-1 (D index does not contain an s)\n",
    "        GderivInit = G[D](s0,deriv=True)\n",
    "        lossderiv = loss(Nx,Y[i,:],cost_type,deriv = True)\n",
    "        #-----------------------------------------------------------------\n",
    "        #deltInit =  np.multiply(lossderiv, GderivInit) # HADAMARD PRODUCT\n",
    "        #print(deltInit)\n",
    "        #-----------------------------------------------------------------\n",
    "        deltInit =  lossderiv * GderivInit\n",
    "        #add to dictionary\n",
    "        deltas_dict[i][D-1] = deltInit #goes up to D-1 only\n",
    "        \n",
    "        #next delta^(l) from l=D-2 to l=0\n",
    "        for l in range(D-2,-1,-1):\n",
    "            \n",
    "            #calculate next delta\n",
    "            prevdelta = deltas_dict[i][l+1] \n",
    "            sl = X_feeds[i][l][1]           #SOMETHING WRONG HERE -GOEST TO [nan nan nan nan]\n",
    "            #print(sl)\n",
    "            Gderiv = G[l+1](sl,deriv=True) # GOES TO ZERO IN PROB 4 - *****SOMETHING WRONG WITH Gderiv******\n",
    "            #---------------------------------------------------------------------\n",
    "            #newdelta = np.multiply(prevdelta @ W[l+1], Gderiv) # HADAMARD PRODUCT (performs the same)\n",
    "            #print(W[l+1].T.shape, prevdelta.shape, Gderiv)\n",
    "            #---------------------------------------------------------------------\n",
    "            newdelta = (W[l+1].T @ prevdelta) * Gderiv \n",
    "            #add new delta to dictionary\n",
    "            deltas_dict[i][l] = newdelta \n",
    "  \n",
    "    return deltas_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads(X,Y,W,B,G,batch, lambda_ = 0, verbose = False,cost_type = 'se'):\n",
    "    ######################### your code goes here ########################\n",
    "\n",
    "    # X_feeds dictionary\n",
    "    X_feeds = {}\n",
    "    [X_feeds.setdefault(k,feedforward(W,B,G,X[k,:])) for k in batch] \n",
    "    \n",
    "    # -------------------X_feeds ordering?-NO-------------------------------\n",
    "    #new_keys = [i for i in range(len(batch))]\n",
    "    #X_feeds = dict(zip(new_keys, list(X_feeds.values())))\n",
    "\n",
    "    # X_deltas\n",
    "    X_deltas = deltas(X_feeds,Y,W,B,G,verbose,cost_type) # index1:batch, index2:layer\n",
    "    \n",
    "    #------------------cost function derivative with respect to W--------------------------------------\n",
    "    dWs = []\n",
    "    D = len(G)-1 #D=5\n",
    "    for l in range(D): # l:0 to 4 \n",
    "        \n",
    "        # ----------DEBUGGING PRINT STATEMENTS--------------------------\n",
    "        #size for batch = 0\n",
    "        #print(np.shape(X_deltas[0][l]), np.shape(X_feeds[0][l][0]))\n",
    "        #size for batch = 1\n",
    "        #print(np.shape(X_deltas[1][l]), np.shape(X_feeds[1][l][0]))\n",
    "        #---------------------------------------------------------------\n",
    "\n",
    "        product = [np.outer(X_deltas[i][l], X_feeds[i][l][0]) for i in batch] \n",
    "        \n",
    "        #----------DEBUG-------------------------------------------------\n",
    "        #batch 0 product + batch 1 product\n",
    "        #print([matrix.shape for matrix in product])\n",
    "        #----------------------------------------------------------------\n",
    "     \n",
    "        outer_prodsum = sum(product) \n",
    "        dWs.append( (1/len(batch)) *  outer_prodsum + lambda_*2*W[l]) # dW is a list of matrices for each layer, l\n",
    "                \n",
    "    #--------------cost function derivative with respect to B--------------------------------------------\n",
    "    dBs = []\n",
    "    \n",
    "    for l in range(D):\n",
    "        delta_list = [X_deltas[i][l] for i in batch]\n",
    "            \n",
    "        dBs.append((1/len(batch))*sum(delta_list))\n",
    "    \n",
    "    return dWs, dBs, X_feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iters should be a multiple of 100\n",
    "def fit(X,Y,arch,G,alpha = 1e-9, momentum = .01, batch_size = 100, \n",
    "        lambda_ = 0, max_iters = 100,verbose = False, cost_type = 'se',print_costs = True):\n",
    "    ######################### your code goes here ########################\n",
    "    W,B,VW,VB = [],[],[],[]\n",
    "    D,m = len(G)-1, len(X)\n",
    "    \n",
    "    #------------------------initializations----------------------------------------\n",
    "    for l in range(D):\n",
    "        # ---for W: sample N(X;0, 2/(n_l-1 + n_l))\n",
    "        sigma = 2/(arch[l]+arch[l+1])\n",
    "        W.append(np.random.normal(0,sigma,(arch[l+1], arch[l])))\n",
    "        #W.append(np.ones((arch[l+1], arch[l])))\n",
    "        B.append(np.zeros(arch[l+1]))\n",
    "        #B.append(np.array(range(arch[l+1])))\n",
    "        VW.append(np.zeros(W[l].shape))\n",
    "        VB.append(np.zeros(B[l].shape))\n",
    "    #----------------------------------------------------------------------------\n",
    "    \n",
    "    #gradient descent    \n",
    "    epochs = 0\n",
    "    grad_norms = []\n",
    "    while epochs <= max_iters:\n",
    "        \n",
    "        batch = random.sample(range(m),batch_size)\n",
    "        #batch = range(m)[:batch_size] \n",
    "        \n",
    "        # -----------gradients and feeds (updated W and B each iteration)---------------\n",
    "        \n",
    "        #ValueError: operands could not be broadcast together with shapes (len(arch[1]),) (0,) \n",
    "        #In new_delta within deltas()<grads()\n",
    "        dWs,dBs,feeds = grads(X,Y,W,B,G,batch,lambda_,verbose,cost_type) # ERROR AFTER A FEW ITERATIONS OF WHILE LOOP\n",
    "        \n",
    "        #------------gradient norms-----------------------------\n",
    "        norm_sum = sum([np.linalg.norm(dWs[l]) + np.linalg.norm(dBs[l]) for l in range(D)])\n",
    "        grad_norms.append(norm_sum)\n",
    "        \n",
    "        # -----------update values------------------\n",
    "        for l in range(D):\n",
    "\n",
    "            #VW[l] = momentum*VW[l] - alpha*dWs[l]\n",
    "            #VB[l] = momentum*VB[l] - alpha*dBs[l]\n",
    "            \n",
    "            W[l] = W[l] + momentum*VW[l] - alpha*dWs[l]\n",
    "            B[l] = B[l] + momentum*VB[l] - alpha*dBs[l]   \n",
    "        \n",
    "        #-----------costs--------------------------------------\n",
    "        \n",
    "        costs = [loss(feeds[i][D][0],Y[i],cost_type) for i in batch]\n",
    "        \n",
    "        '''\n",
    "        if epochs%(np.floor(max_iters/30))==0 and print_costs:               \n",
    "            print(f'epoch: {epochs}')\n",
    "            print(f'           cost: {costs[epochs]}')\n",
    "         '''  \n",
    "        epochs+=1\n",
    "        \n",
    "    return W,B,costs,grad_norms\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
